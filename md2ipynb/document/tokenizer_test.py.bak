# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements. See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership. The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License. You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied. See the License for the
# specific language governing permissions and limitations
# under the License.

import unittest

from .token import Token, TokenType
from .tokenizer import Tokenizer

t = Tokenizer()


class TokenizerTest(unittest.TestCase):
    def test_tokenize_empty(self) -> None:
        self.assertEqual(list(t.tokenize([])), [Token.EOF('', 1, 1)])

    def test_tokenize_normal_text(self) -> None:
        source = ['normal text']
        self.assertEqual(
            list(t.tokenize(source)),
            [
                Token.Text('normal text', source[0], 1, 1),
                Token.EOF(source[0], 1, 12)
            ])

    def test_tokenize_normal_text_inbetween(self) -> None:
        source = ['<b>bold</b> text']
        self.assertEqual(
            list(t.tokenize(source)),
            [
                Token.HtmlOpen('<b>', source[0], 1, 1),
                Token.Text('bold', source[0], 1, 4),
                Token.HtmlClose('</b>', source[0], 1, 8),
                Token.Text(' text', source[0], 1, 12),
                Token.EOF(source[0], 1, 17),
            ])

    def test_tokenize_multiple_lines(self) -> None:
        source = ['multiple<br>', 'lines<br>', ':)<br>']
        self.assertEqual(
            list(t.tokenize(source)),
            [
                Token.Text('multiple', source[0], 1, 1),
                Token.HtmlOpen('<br>', source[0], 1, 9),
                Token.Text('lines', source[1], 2, 1),
                Token.HtmlOpen('<br>', source[1], 2, 6),
                Token.Text(':)', source[2], 3, 1),
                Token.HtmlOpen('<br>', source[2], 3, 3),
                Token.EOF(source[2], 3, 7),
            ])

    def test_tokenize_h1(self) -> None:
        source = ['# H1']
        self.assertEqual(
            list(t.tokenize(source)),
            [
                Token.H1('#', source[0], 1, 1),
                Token.Text(' H1', source[0], 1, 2),
                Token.EOF(source[0], 1, 5),
            ])

    def test_tokenize_h2(self) -> None:
        source = ['## H2']
        self.assertEqual(
            list(t.tokenize(source)),
            [
                Token.H2('##', source[0], 1, 1),
                Token.Text(' H2', source[0], 1, 3),
                Token.EOF(source[0], 1, 6),
            ])

    def test_tokenize_h3(self) -> None:
        source = ['### H3']
        self.assertEqual(
            list(t.tokenize(source)),
            [
                Token.H3('###', source[0], 1, 1),
                Token.Text(' H3', source[0], 1, 4),
                Token.EOF(source[0], 1, 7),
            ])

    def test_tokenize_h4(self) -> None:
        source = ['#### H4']
        self.assertEqual(
            list(t.tokenize(source)),
            [
                Token.H4('####', source[0], 1, 1),
                Token.Text(' H4', source[0], 1, 5),
                Token.EOF(source[0], 1, 8),
            ])

    def test_tokenize_h5(self) -> None:
        source = ['##### H5']
        self.assertEqual(
            list(t.tokenize(source)),
            [
                Token.H5('#####', source[0], 1, 1),
                Token.Text(' H5', source[0], 1, 6),
                Token.EOF(source[0], 1, 9),
            ])

    def test_tokenize_h6(self) -> None:
        source = ['###### H6']
        self.assertEqual(
            list(t.tokenize(source)),
            [
                Token.H6('######', source[0], 1, 1),
                Token.Text(' H6', source[0], 1, 7),
                Token.EOF(source[0], 1, 10),
            ])

    def test_tokenize_html_open(self) -> None:
        source = ['<b>']
        self.assertEqual(
            list(t.tokenize(source)),
            [
                Token.HtmlOpen('<b>', source[0], 1, 1),
                Token.EOF(source[0], 1, 4)
            ])

    def test_tokenize_html_close(self) -> None:
        source = ['</b>']
        self.assertEqual(
            list(t.tokenize(source)),
            [
                Token.HtmlClose('</b>', source[0], 1, 1),
                Token.EOF(source[0], 1, 5)
            ])

    def test_tokenize_html_comment(self) -> None:
        source = ['text<!-- comment -->text']
        self.assertEqual(
            list(t.tokenize(source)),
            [
                Token.Text('text', source[0], 1, 1),
                Token.HtmlComment('<!-- comment -->', source[0], 1, 5),
                Token.Text('text', source[0], 1, 21),
                Token.EOF(source[0], 1, 25),
            ])

    def test_tokenize_html_comment_multiline(self) -> None:
        source = ['text<!-- multi', 'line', 'comment -->text']
        self.assertEqual(
            list(t.tokenize(source)),
            [
                Token.Text('text', source[0], 1, 1),
                Token.HtmlComment(
                    '<!-- multi\nline\ncomment -->', source[0], 1, 5),
                Token.Text('text', source[2], 3, 12),
                Token.EOF(source[2], 3, 16),
            ])

    def test_tokenize_asterisk(self) -> None:
        source = ['text*text']
        self.assertEqual(
            list(t.tokenize(source)),
            [
                Token.Text('text', source[0], 1, 1),
                Token.Asterisk('*', source[0], 1, 5),
                Token.Text('text', source[0], 1, 6),
                Token.EOF(source[0], 1, 10),
            ])

    def test_tokenize_plus(self) -> None:
        source = ['text+text']
        self.assertEqual(
            list(t.tokenize(source)),
            [
                Token.Text('text', source[0], 1, 1),
                Token.Plus('+', source[0], 1, 5),
                Token.Text('text', source[0], 1, 6),
                Token.EOF(source[0], 1, 10),
            ])

    def test_tokenize_dash(self) -> None:
        source = ['text-text']
        self.assertEqual(
            list(t.tokenize(source)),
            [
                Token.Text('text', source[0], 1, 1),
                Token.Dash('-', source[0], 1, 5),
                Token.Text('text', source[0], 1, 6),
                Token.EOF(source[0], 1, 10),
            ])

    def test_tokenize_underscore(self) -> None:
        source = ['text_text']
        self.assertEqual(
            list(t.tokenize(source)),
            [
                Token.Text('text', source[0], 1, 1),
                Token.Underscore('_', source[0], 1, 5),
                Token.Text('text', source[0], 1, 6),
                Token.EOF(source[0], 1, 10),
            ])

    def test_tokenize_strike(self) -> None:
        source = ['text~~text']
        self.assertEqual(
            list(t.tokenize(source)),
            [
                Token.Text('text', source[0], 1, 1),
                Token.Strike('~~', source[0], 1, 5),
                Token.Text('text', source[0], 1, 7),
                Token.EOF(source[0], 1, 11),
            ])

    def test_tokenize_greater_than(self) -> None:
        source = ['text>text']
        self.assertEqual(
            list(t.tokenize(source)),
            [
                Token.Text('text', source[0], 1, 1),
                Token.GreaterThan('>', source[0], 1, 5),
                Token.Text('text', source[0], 1, 6),
                Token.EOF(source[0], 1, 10),
            ])

    def test_tokenize_ordered_item1(self) -> None:
        source = ['text1.text']
        self.assertEqual(
            list(t.tokenize(source)),
            [
                Token.Text('text', source[0], 1, 1),
                Token.GreaterThan('>', source[0], 1, 5),
                Token.Text('text', source[0], 1, 6),
                Token.EOF(source[0], 1, 10),
            ])
